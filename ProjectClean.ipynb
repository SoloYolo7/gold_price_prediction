{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdd3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tempfile\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –¥–ª—è headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cf0073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow URI: http://84.201.144.227:8000\n",
      "Experiment: financial_timeseries_regression\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://84.201.144.227:8000\")\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "EXPERIMENT_NAME = \"financial_timeseries_regression\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(\"MLflow URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/financial_regression.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.set_index(\"date\")\n",
    "df.dropna(subset=[\"gold close\"], inplace=True)\n",
    "\n",
    "# –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "df[\"year\"] = df.index.year\n",
    "df[\"month\"] = df.index.month\n",
    "df[\"dayofweek\"] = df.index.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a089643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "key_features = [c for c in [\"silver close\", \"oil close\", \"dxy close\"] if c in df_fe.columns]\n",
    "for col in key_features:\n",
    "    df_fe[f\"{col}_lag1\"] = df_fe[col].shift(1)\n",
    "    df_fe[f\"{col}_roll_mean3\"] = df_fe[col].rolling(window=3).mean()\n",
    "\n",
    "df_fe[\"gold_close_lag1\"] = df_fe[\"gold close\"].shift(1)\n",
    "\n",
    "y = df_fe[\"gold close\"]\n",
    "X = df_fe.drop(columns=[\"gold close\"])\n",
    "\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# train/test –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "feature_names = X_train_raw.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6397deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    m = {\n",
    "        \"mae\":  mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\":  mean_squared_error(y_true, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"r2\":   r2_score(y_true, y_pred),\n",
    "        \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "    }\n",
    "    return m\n",
    "\n",
    "def log_metrics_dict(mdict):\n",
    "    for k, v in mdict.items():\n",
    "        mlflow.log_metric(k, float(v))\n",
    "\n",
    "# =========================\n",
    "#  –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "# =========================\n",
    "def save_predictions(y_true, y_pred, model_name):\n",
    "    out = pd.DataFrame({\"y_true\": y_true.values, \"y_pred\": np.asarray(y_pred)}, index=y_true.index)\n",
    "    fname = f\"{model_name}_predictions.csv\"\n",
    "    out.to_csv(fname)\n",
    "    mlflow.log_artifact(fname)\n",
    "\n",
    "def log_feature_importances(estimator_or_pipeline, model_name, names):\n",
    "    # –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏ pipeline, –∏ ¬´–≥–æ–ª—É—é¬ª –º–æ–¥–µ–ª—å\n",
    "    if hasattr(estimator_or_pipeline, \"named_steps\"):\n",
    "        model = estimator_or_pipeline.named_steps.get(\"model\", estimator_or_pipeline)\n",
    "    else:\n",
    "        model = estimator_or_pipeline\n",
    "\n",
    "    importances = None\n",
    "    # CatBoost\n",
    "    if hasattr(model, \"get_feature_importance\"):\n",
    "        try:\n",
    "            importances = np.asarray(model.get_feature_importance(), dtype=float)\n",
    "        except Exception:\n",
    "            importances = None\n",
    "    # XGB/LGBM/sklearn\n",
    "    if importances is None and hasattr(model, \"feature_importances_\"):\n",
    "        importances = np.asarray(model.feature_importances_, dtype=float)\n",
    "\n",
    "    if importances is None:\n",
    "        print(f\"[{model_name}] feature importances not available ‚Äî skipped.\")\n",
    "        return\n",
    "\n",
    "    fi = pd.DataFrame({\"feature\": names, \"importance\": importances})\n",
    "    fi = fi.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    csv_name = f\"feature_importances_{model_name}.csv\"\n",
    "    fi.to_csv(csv_name, index=False)\n",
    "    mlflow.log_artifact(csv_name)\n",
    "\n",
    "    top = fi.head(30).iloc[::-1]\n",
    "    plt.figure(figsize=(10, max(6, len(top)*0.3)))\n",
    "    plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "    plt.title(f\"Feature importances ‚Äî {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    png_name = f\"feature_importances_{model_name}.png\"\n",
    "    plt.savefig(png_name, dpi=150)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(png_name)\n",
    "\n",
    "def log_summary_text(model_name, params, metrics):\n",
    "    lines = [f\"Model: {model_name}\", \"Params:\"]\n",
    "    for k, v in (params or {}).items():\n",
    "        lines.append(f\"  {k}: {v}\")\n",
    "    lines.append(\"Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        lines.append(f\"  {k}: {v}\")\n",
    "    txt = \"\\n\".join(lines)\n",
    "    fname = f\"{model_name}_summary.txt\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    mlflow.log_artifact(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288c6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgb(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ddd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": -1,\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3d223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cat(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 150, 400),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 8),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True),\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd465ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_run(model_name, pipeline, params):\n",
    "    \"\"\"\n",
    "    –û—Ç–¥–µ–ª—å–Ω—ã–π run –Ω–∞ –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å:\n",
    "      - –ª–æ–≥–∏—Ä—É–µ–º params/metrics/–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (predictions/FI/summary)\n",
    "      - –∏ –ì–õ–ê–í–ù–û–ï: —Å–æ–∑–¥–∞—ë–º –≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö –ø–∞–ø–∫—É model_pipeline —Å MLmodel/conda/reqs/model.pkl\n",
    "        –ª–∏–±–æ —á–µ—Ä–µ–∑ log_model, –ª–∏–±–æ —á–µ—Ä–µ–∑ save_model + log_artifacts (fallback).\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_Final\"):\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "        # fit ‚Üí predict\n",
    "        pipeline.fit(X_train_raw, y_train)\n",
    "        y_pred = pipeline.predict(X_test_raw)\n",
    "\n",
    "        # –º–µ—Ç—Ä–∏–∫–∏ + –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã\n",
    "        metrics = calc_metrics(y_test, y_pred)\n",
    "        log_metrics_dict(metrics)\n",
    "        save_predictions(y_test, y_pred, model_name)\n",
    "        log_feature_importances(pipeline, model_name, feature_names)\n",
    "        log_summary_text(model_name, params, metrics)\n",
    "\n",
    "        # –ø–æ–¥–ø–∏—Å—å –∏ –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞\n",
    "        input_example = X_test_raw.iloc[:2].copy()\n",
    "        try:\n",
    "            signature = infer_signature(X_train_raw, pipeline.predict(X_train_raw.iloc[:2]))\n",
    "        except Exception:\n",
    "            signature = None\n",
    "\n",
    "        # --- 1) –û–±—ã—á–Ω—ã–π –ø—É—Ç—å (—É —Ç–≤–æ–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ artifact_path) ---\n",
    "        log_ok = False\n",
    "        try:\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=pipeline,\n",
    "                artifact_path=\"model_pipeline\",  # ‚Üê –∫–ª—é—á–µ–≤–∞—è –ø—Ä–∞–≤–∫–∞\n",
    "                input_example=input_example,\n",
    "                signature=signature,\n",
    "                pip_requirements=[\n",
    "                    \"mlflow\",\n",
    "                    \"scikit-learn\",\n",
    "                    \"pandas\",\n",
    "                    \"numpy\",\n",
    "                    \"xgboost\",\n",
    "                    \"lightgbm\",\n",
    "                    \"catboost\",\n",
    "                ],\n",
    "            )\n",
    "            log_ok = True\n",
    "        except Exception:\n",
    "            # —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–∏—á–∏–Ω—É –ø—Ä—è–º–æ –≤ UI\n",
    "            err_txt = \"[log_model] failed:\\n\" + traceback.format_exc()\n",
    "            with open(\"model_log_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(err_txt)\n",
    "            mlflow.log_artifact(\"model_log_error.txt\")\n",
    "            print(err_txt)\n",
    "\n",
    "        # --- 2) –ù–∞–¥—ë–∂–Ω—ã–π fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–ø–∫—É –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã ---\n",
    "        if not log_ok:\n",
    "            try:\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    local_dir = os.path.join(tmpdir, \"model_pipeline\")\n",
    "                    mlflow.sklearn.save_model(\n",
    "                        sk_model=pipeline,\n",
    "                        path=local_dir,\n",
    "                        input_example=input_example,\n",
    "                        signature=signature,\n",
    "                        pip_requirements=[\n",
    "                            \"mlflow\",\n",
    "                            \"scikit-learn\",\n",
    "                            \"pandas\",\n",
    "                            \"numpy\",\n",
    "                            \"xgboost\",\n",
    "                            \"lightgbm\",\n",
    "                            \"catboost\",\n",
    "                        ],\n",
    "                    )\n",
    "                    mlflow.log_artifacts(local_dir, artifact_path=\"model_pipeline\")\n",
    "                log_ok = True\n",
    "            except Exception:\n",
    "                err_txt = \"[save_model/log_artifacts] fallback failed:\\n\" + traceback.format_exc()\n",
    "                with open(\"model_save_fallback_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(err_txt)\n",
    "                mlflow.log_artifact(\"model_save_fallback_error.txt\")\n",
    "                print(err_txt)\n",
    "\n",
    "        print(f\"[{model_name}] run finished. model_pipeline logged: {log_ok}\")\n",
    "        print(\n",
    "            f\"üèÉ View run {model_name}_Final at: \"\n",
    "            f\"{mlflow.get_tracking_uri()}/#/experiments/{mlflow.active_run().info.experiment_id}\"\n",
    "            f\"/runs/{mlflow.active_run().info.run_id}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62367d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:15,574] A new study created in memory with name: no-name-f363309f-c933-4bc2-b66d-be285f2034a4\n",
      "[I 2025-09-01 22:04:16,116] Trial 0 finished with value: 2.78359871190345 and parameters: {'n_estimators': 179, 'learning_rate': 0.014747928982506812, 'max_depth': 3, 'subsample': 0.6660004190962207, 'colsample_bytree': 0.9846679329735166, 'reg_alpha': 0.4562000844424491, 'reg_lambda': 0.12740576177106844}. Best is trial 0 with value: 2.78359871190345.\n",
      "[I 2025-09-01 22:04:16,734] Trial 1 finished with value: 1.7049174154956528 and parameters: {'n_estimators': 121, 'learning_rate': 0.05701492261656593, 'max_depth': 4, 'subsample': 0.9572066117333473, 'colsample_bytree': 0.8331538236564447, 'reg_alpha': 0.6457936851028607, 'reg_lambda': 0.08881548573217546}. Best is trial 1 with value: 1.7049174154956528.\n",
      "[I 2025-09-01 22:04:17,391] Trial 2 finished with value: 1.6915009615019787 and parameters: {'n_estimators': 180, 'learning_rate': 0.08266511135663787, 'max_depth': 4, 'subsample': 0.8012464966020458, 'colsample_bytree': 0.7273573527853789, 'reg_alpha': 0.02504436357218487, 'reg_lambda': 0.024798903785407944}. Best is trial 2 with value: 1.6915009615019787.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad68a38dd44943b2b09daa2978f16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:23,462] A new study created in memory with name: no-name-5fee5e83-3043-4e1b-8126-156c48ea25b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost_Optuna] run finished. model_pipeline logged: True\n",
      "üèÉ View run XGBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/94ae8c757e82422c82f11493c3644ab3\n",
      "üèÉ View run XGBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/94ae8c757e82422c82f11493c3644ab3\n",
      "üß™ View experiment at: http://84.201.144.227:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:25,508] Trial 0 finished with value: 1.6293655599354573 and parameters: {'n_estimators': 274, 'learning_rate': 0.06687613950735954, 'num_leaves': 47, 'max_depth': 4, 'subsample': 0.840243536795265, 'colsample_bytree': 0.7563153100182392}. Best is trial 0 with value: 1.6293655599354573.\n",
      "[I 2025-09-01 22:04:25,656] Trial 1 finished with value: 2.9201953340479414 and parameters: {'n_estimators': 102, 'learning_rate': 0.027638259217635384, 'num_leaves': 30, 'max_depth': 3, 'subsample': 0.6780897125215873, 'colsample_bytree': 0.6129877320478468}. Best is trial 0 with value: 1.6293655599354573.\n",
      "[I 2025-09-01 22:04:26,032] Trial 2 finished with value: 2.2452132780586918 and parameters: {'n_estimators': 228, 'learning_rate': 0.05529916447594308, 'num_leaves': 47, 'max_depth': 5, 'subsample': 0.7577887640787003, 'colsample_bytree': 0.6209573317083622}. Best is trial 0 with value: 1.6293655599354573.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5369f032a24202849b47dc746cda63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:28,169] A new study created in memory with name: no-name-fd24c5aa-4589-44d1-a628-2918d16b9025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM_Optuna] run finished. model_pipeline logged: True\n",
      "üèÉ View run LightGBM_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/36bd89f46a6b43d4b5c649a6fb47c8ec\n",
      "üèÉ View run LightGBM_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/36bd89f46a6b43d4b5c649a6fb47c8ec\n",
      "üß™ View experiment at: http://84.201.144.227:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:30,614] Trial 0 finished with value: 9.357086148482727 and parameters: {'iterations': 238, 'depth': 5, 'learning_rate': 0.043580966555352575, 'l2_leaf_reg': 3.1040296902047966}. Best is trial 0 with value: 9.357086148482727.\n",
      "[I 2025-09-01 22:04:45,621] Trial 1 finished with value: 15.331509658344203 and parameters: {'iterations': 310, 'depth': 8, 'learning_rate': 0.0930312128731187, 'l2_leaf_reg': 4.518059146868752}. Best is trial 0 with value: 9.357086148482727.\n",
      "[I 2025-09-01 22:04:50,407] Trial 2 finished with value: 12.597224599878032 and parameters: {'iterations': 339, 'depth': 6, 'learning_rate': 0.045993458187334996, 'l2_leaf_reg': 2.042574731619601}. Best is trial 0 with value: 9.357086148482727.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b104448b714a5699e0053926c87d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CatBoost_Optuna] run finished. model_pipeline logged: True\n",
      "üèÉ View run CatBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/eb8a537b0c4e4d298883e790b2dac034\n",
      "üèÉ View run CatBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/eb8a537b0c4e4d298883e790b2dac034\n",
      "üß™ View experiment at: http://84.201.144.227:8000/#/experiments/8\n",
      "=== All done ===\n"
     ]
    }
   ],
   "source": [
    "best_xgb = tune_xgb(n_trials=3, timeout=300)\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42, **best_xgb))\n",
    "])\n",
    "final_run(\"XGBoost_Optuna\", xgb_pipe, best_xgb)\n",
    "\n",
    "# LightGBM\n",
    "best_lgbm = tune_lgbm(n_trials=3, timeout=300)\n",
    "lgbm_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", LGBMRegressor(random_state=42, **best_lgbm))\n",
    "])\n",
    "final_run(\"LightGBM_Optuna\", lgbm_pipe, best_lgbm)\n",
    "\n",
    "# CatBoost\n",
    "best_cat = tune_cat(n_trials=3, timeout=300)\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", CatBoostRegressor(verbose=0, random_state=42, **best_cat))\n",
    "])\n",
    "final_run(\"CatBoost_Optuna\", cat_pipe, best_cat)\n",
    "\n",
    "print(\"=== All done ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
