{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdd3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tempfile\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")  # на всякий случай для headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cf0073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow URI: http://84.201.144.227:8000\n",
      "Experiment: financial_timeseries_regression\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://84.201.144.227:8000\")\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "EXPERIMENT_NAME = \"financial_timeseries_regression\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(\"MLflow URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/financial_regression.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.set_index(\"date\")\n",
    "df.dropna(subset=[\"gold close\"], inplace=True)\n",
    "\n",
    "# календарные признаки\n",
    "df[\"year\"] = df.index.year\n",
    "df[\"month\"] = df.index.month\n",
    "df[\"dayofweek\"] = df.index.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a089643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "key_features = [c for c in [\"silver close\", \"oil close\", \"dxy close\"] if c in df_fe.columns]\n",
    "for col in key_features:\n",
    "    df_fe[f\"{col}_lag1\"] = df_fe[col].shift(1)\n",
    "    df_fe[f\"{col}_roll_mean3\"] = df_fe[col].rolling(window=3).mean()\n",
    "\n",
    "df_fe[\"gold_close_lag1\"] = df_fe[\"gold close\"].shift(1)\n",
    "\n",
    "y = df_fe[\"gold close\"]\n",
    "X = df_fe.drop(columns=[\"gold close\"])\n",
    "\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# train/test по времени (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "feature_names = X_train_raw.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6397deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    m = {\n",
    "        \"mae\":  mean_absolute_error(y_true, y_pred),\n",
    "        \"mse\":  mean_squared_error(y_true, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"r2\":   r2_score(y_true, y_pred),\n",
    "        \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "    }\n",
    "    return m\n",
    "\n",
    "def log_metrics_dict(mdict):\n",
    "    for k, v in mdict.items():\n",
    "        mlflow.log_metric(k, float(v))\n",
    "\n",
    "# =========================\n",
    "#  Вспомогательные артефакты\n",
    "# =========================\n",
    "def save_predictions(y_true, y_pred, model_name):\n",
    "    out = pd.DataFrame({\"y_true\": y_true.values, \"y_pred\": np.asarray(y_pred)}, index=y_true.index)\n",
    "    fname = f\"{model_name}_predictions.csv\"\n",
    "    out.to_csv(fname)\n",
    "    mlflow.log_artifact(fname)\n",
    "\n",
    "def log_feature_importances(estimator_or_pipeline, model_name, names):\n",
    "    # принимает и pipeline, и «голую» модель\n",
    "    if hasattr(estimator_or_pipeline, \"named_steps\"):\n",
    "        model = estimator_or_pipeline.named_steps.get(\"model\", estimator_or_pipeline)\n",
    "    else:\n",
    "        model = estimator_or_pipeline\n",
    "\n",
    "    importances = None\n",
    "    # CatBoost\n",
    "    if hasattr(model, \"get_feature_importance\"):\n",
    "        try:\n",
    "            importances = np.asarray(model.get_feature_importance(), dtype=float)\n",
    "        except Exception:\n",
    "            importances = None\n",
    "    # XGB/LGBM/sklearn\n",
    "    if importances is None and hasattr(model, \"feature_importances_\"):\n",
    "        importances = np.asarray(model.feature_importances_, dtype=float)\n",
    "\n",
    "    if importances is None:\n",
    "        print(f\"[{model_name}] feature importances not available — skipped.\")\n",
    "        return\n",
    "\n",
    "    fi = pd.DataFrame({\"feature\": names, \"importance\": importances})\n",
    "    fi = fi.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    csv_name = f\"feature_importances_{model_name}.csv\"\n",
    "    fi.to_csv(csv_name, index=False)\n",
    "    mlflow.log_artifact(csv_name)\n",
    "\n",
    "    top = fi.head(30).iloc[::-1]\n",
    "    plt.figure(figsize=(10, max(6, len(top)*0.3)))\n",
    "    plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "    plt.title(f\"Feature importances — {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    png_name = f\"feature_importances_{model_name}.png\"\n",
    "    plt.savefig(png_name, dpi=150)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(png_name)\n",
    "\n",
    "def log_summary_text(model_name, params, metrics):\n",
    "    lines = [f\"Model: {model_name}\", \"Params:\"]\n",
    "    for k, v in (params or {}).items():\n",
    "        lines.append(f\"  {k}: {v}\")\n",
    "    lines.append(\"Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        lines.append(f\"  {k}: {v}\")\n",
    "    txt = \"\\n\".join(lines)\n",
    "    fname = f\"{model_name}_summary.txt\"\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "    mlflow.log_artifact(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288c6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgb(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ddd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": -1,\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3d223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cat(n_trials=10, timeout=600):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 150, 400),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 8),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True),\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in tscv.split(X_train_raw):\n",
    "            pipe.fit(X_train_raw.iloc[tr_idx], y_train.iloc[tr_idx])\n",
    "            pred = pipe.predict(X_train_raw.iloc[val_idx])\n",
    "            rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], pred))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd465ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_run(model_name, pipeline, params):\n",
    "    \"\"\"\n",
    "    Отдельный run на каждую модель:\n",
    "      - логируем params/metrics/артефакты (predictions/FI/summary)\n",
    "      - и ГЛАВНОЕ: создаём в артефактах папку model_pipeline с MLmodel/conda/reqs/model.pkl\n",
    "        либо через log_model, либо через save_model + log_artifacts (fallback).\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_Final\"):\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "        # fit → predict\n",
    "        pipeline.fit(X_train_raw, y_train)\n",
    "        y_pred = pipeline.predict(X_test_raw)\n",
    "\n",
    "        # метрики + артефакты\n",
    "        metrics = calc_metrics(y_test, y_pred)\n",
    "        log_metrics_dict(metrics)\n",
    "        save_predictions(y_test, y_pred, model_name)\n",
    "        log_feature_importances(pipeline, model_name, feature_names)\n",
    "        log_summary_text(model_name, params, metrics)\n",
    "\n",
    "        # подпись и пример входа\n",
    "        input_example = X_test_raw.iloc[:2].copy()\n",
    "        try:\n",
    "            signature = infer_signature(X_train_raw, pipeline.predict(X_train_raw.iloc[:2]))\n",
    "        except Exception:\n",
    "            signature = None\n",
    "\n",
    "        # --- 1) Обычный путь (у твоего клиента работает только artifact_path) ---\n",
    "        log_ok = False\n",
    "        try:\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=pipeline,\n",
    "                artifact_path=\"model_pipeline\",  # ← ключевая правка\n",
    "                input_example=input_example,\n",
    "                signature=signature,\n",
    "                pip_requirements=[\n",
    "                    \"mlflow\",\n",
    "                    \"scikit-learn\",\n",
    "                    \"pandas\",\n",
    "                    \"numpy\",\n",
    "                    \"xgboost\",\n",
    "                    \"lightgbm\",\n",
    "                    \"catboost\",\n",
    "                ],\n",
    "            )\n",
    "            log_ok = True\n",
    "        except Exception:\n",
    "            # чтобы видеть причину прямо в UI\n",
    "            err_txt = \"[log_model] failed:\\n\" + traceback.format_exc()\n",
    "            with open(\"model_log_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(err_txt)\n",
    "            mlflow.log_artifact(\"model_log_error.txt\")\n",
    "            print(err_txt)\n",
    "\n",
    "        # --- 2) Надёжный fallback: сохраняем локально и загружаем папку как артефакты ---\n",
    "        if not log_ok:\n",
    "            try:\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    local_dir = os.path.join(tmpdir, \"model_pipeline\")\n",
    "                    mlflow.sklearn.save_model(\n",
    "                        sk_model=pipeline,\n",
    "                        path=local_dir,\n",
    "                        input_example=input_example,\n",
    "                        signature=signature,\n",
    "                        pip_requirements=[\n",
    "                            \"mlflow\",\n",
    "                            \"scikit-learn\",\n",
    "                            \"pandas\",\n",
    "                            \"numpy\",\n",
    "                            \"xgboost\",\n",
    "                            \"lightgbm\",\n",
    "                            \"catboost\",\n",
    "                        ],\n",
    "                    )\n",
    "                    mlflow.log_artifacts(local_dir, artifact_path=\"model_pipeline\")\n",
    "                log_ok = True\n",
    "            except Exception:\n",
    "                err_txt = \"[save_model/log_artifacts] fallback failed:\\n\" + traceback.format_exc()\n",
    "                with open(\"model_save_fallback_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(err_txt)\n",
    "                mlflow.log_artifact(\"model_save_fallback_error.txt\")\n",
    "                print(err_txt)\n",
    "\n",
    "        print(f\"[{model_name}] run finished. model_pipeline logged: {log_ok}\")\n",
    "        print(\n",
    "            f\"🏃 View run {model_name}_Final at: \"\n",
    "            f\"{mlflow.get_tracking_uri()}/#/experiments/{mlflow.active_run().info.experiment_id}\"\n",
    "            f\"/runs/{mlflow.active_run().info.run_id}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62367d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:15,574] A new study created in memory with name: no-name-f363309f-c933-4bc2-b66d-be285f2034a4\n",
      "[I 2025-09-01 22:04:16,116] Trial 0 finished with value: 2.78359871190345 and parameters: {'n_estimators': 179, 'learning_rate': 0.014747928982506812, 'max_depth': 3, 'subsample': 0.6660004190962207, 'colsample_bytree': 0.9846679329735166, 'reg_alpha': 0.4562000844424491, 'reg_lambda': 0.12740576177106844}. Best is trial 0 with value: 2.78359871190345.\n",
      "[I 2025-09-01 22:04:16,734] Trial 1 finished with value: 1.7049174154956528 and parameters: {'n_estimators': 121, 'learning_rate': 0.05701492261656593, 'max_depth': 4, 'subsample': 0.9572066117333473, 'colsample_bytree': 0.8331538236564447, 'reg_alpha': 0.6457936851028607, 'reg_lambda': 0.08881548573217546}. Best is trial 1 with value: 1.7049174154956528.\n",
      "[I 2025-09-01 22:04:17,391] Trial 2 finished with value: 1.6915009615019787 and parameters: {'n_estimators': 180, 'learning_rate': 0.08266511135663787, 'max_depth': 4, 'subsample': 0.8012464966020458, 'colsample_bytree': 0.7273573527853789, 'reg_alpha': 0.02504436357218487, 'reg_lambda': 0.024798903785407944}. Best is trial 2 with value: 1.6915009615019787.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad68a38dd44943b2b09daa2978f16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:23,462] A new study created in memory with name: no-name-5fee5e83-3043-4e1b-8126-156c48ea25b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost_Optuna] run finished. model_pipeline logged: True\n",
      "🏃 View run XGBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/94ae8c757e82422c82f11493c3644ab3\n",
      "🏃 View run XGBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/94ae8c757e82422c82f11493c3644ab3\n",
      "🧪 View experiment at: http://84.201.144.227:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:25,508] Trial 0 finished with value: 1.6293655599354573 and parameters: {'n_estimators': 274, 'learning_rate': 0.06687613950735954, 'num_leaves': 47, 'max_depth': 4, 'subsample': 0.840243536795265, 'colsample_bytree': 0.7563153100182392}. Best is trial 0 with value: 1.6293655599354573.\n",
      "[I 2025-09-01 22:04:25,656] Trial 1 finished with value: 2.9201953340479414 and parameters: {'n_estimators': 102, 'learning_rate': 0.027638259217635384, 'num_leaves': 30, 'max_depth': 3, 'subsample': 0.6780897125215873, 'colsample_bytree': 0.6129877320478468}. Best is trial 0 with value: 1.6293655599354573.\n",
      "[I 2025-09-01 22:04:26,032] Trial 2 finished with value: 2.2452132780586918 and parameters: {'n_estimators': 228, 'learning_rate': 0.05529916447594308, 'num_leaves': 47, 'max_depth': 5, 'subsample': 0.7577887640787003, 'colsample_bytree': 0.6209573317083622}. Best is trial 0 with value: 1.6293655599354573.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5369f032a24202849b47dc746cda63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:28,169] A new study created in memory with name: no-name-fd24c5aa-4589-44d1-a628-2918d16b9025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM_Optuna] run finished. model_pipeline logged: True\n",
      "🏃 View run LightGBM_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/36bd89f46a6b43d4b5c649a6fb47c8ec\n",
      "🏃 View run LightGBM_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/36bd89f46a6b43d4b5c649a6fb47c8ec\n",
      "🧪 View experiment at: http://84.201.144.227:8000/#/experiments/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 22:04:30,614] Trial 0 finished with value: 9.357086148482727 and parameters: {'iterations': 238, 'depth': 5, 'learning_rate': 0.043580966555352575, 'l2_leaf_reg': 3.1040296902047966}. Best is trial 0 with value: 9.357086148482727.\n",
      "[I 2025-09-01 22:04:45,621] Trial 1 finished with value: 15.331509658344203 and parameters: {'iterations': 310, 'depth': 8, 'learning_rate': 0.0930312128731187, 'l2_leaf_reg': 4.518059146868752}. Best is trial 0 with value: 9.357086148482727.\n",
      "[I 2025-09-01 22:04:50,407] Trial 2 finished with value: 12.597224599878032 and parameters: {'iterations': 339, 'depth': 6, 'learning_rate': 0.045993458187334996, 'l2_leaf_reg': 2.042574731619601}. Best is trial 0 with value: 9.357086148482727.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b104448b714a5699e0053926c87d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CatBoost_Optuna] run finished. model_pipeline logged: True\n",
      "🏃 View run CatBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/eb8a537b0c4e4d298883e790b2dac034\n",
      "🏃 View run CatBoost_Optuna_Final at: http://84.201.144.227:8000/#/experiments/8/runs/eb8a537b0c4e4d298883e790b2dac034\n",
      "🧪 View experiment at: http://84.201.144.227:8000/#/experiments/8\n",
      "=== All done ===\n"
     ]
    }
   ],
   "source": [
    "best_xgb = tune_xgb(n_trials=3, timeout=300)\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42, **best_xgb))\n",
    "])\n",
    "final_run(\"XGBoost_Optuna\", xgb_pipe, best_xgb)\n",
    "\n",
    "# LightGBM\n",
    "best_lgbm = tune_lgbm(n_trials=3, timeout=300)\n",
    "lgbm_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", LGBMRegressor(random_state=42, **best_lgbm))\n",
    "])\n",
    "final_run(\"LightGBM_Optuna\", lgbm_pipe, best_lgbm)\n",
    "\n",
    "# CatBoost\n",
    "best_cat = tune_cat(n_trials=3, timeout=300)\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"model\", CatBoostRegressor(verbose=0, random_state=42, **best_cat))\n",
    "])\n",
    "final_run(\"CatBoost_Optuna\", cat_pipe, best_cat)\n",
    "\n",
    "print(\"=== All done ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
